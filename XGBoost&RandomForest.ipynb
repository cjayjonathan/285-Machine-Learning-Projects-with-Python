{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpbqdzYedP/9w52NP3QXdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjayjonathan/285-Machine-Learning-Projects-with-Python/blob/main/XGBoost%26RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are using the load_breast_cancer dataset from scikit-learn to train a binary classification model with XGBoost. We split the data into training and test sets using the train_test_split function, and then set up an XGBoost classifier with some hyperparameters like n_estimators, max_depth, and learning_rate. We then train the model with the training data using the fit method, and evaluate its performance on the training and test sets using the score method."
      ],
      "metadata": {
        "id": "otSlIlDS3OWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5ytyWg3LP_",
        "outputId": "f81627c5-ffcd-4a62-d40b-5e102830d714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.995\n",
            "Accuracy on test set: 0.965\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Set up the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, subsample=0.5, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy on training set: {:.3f}\".format(xgb_clf.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(xgb_clf.score(X_test, y_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are using the same load_breast_cancer dataset to train a Random Forest model. We split the data into training and test sets using the train_test_split function, and then set up a Random Forest classifier with some hyperparameters like n_estimators and max_depth. We then train the model with the training data using the fit method, and evaluate its performance on the training and test sets using the score method.\n"
      ],
      "metadata": {
        "id": "WlYptesc3Mjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Set up the Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy on training set: {:.3f}\".format(rf_clf.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(rf_clf.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkaWTNXU3o-z",
        "outputId": "39668d75-0f2c-4614-e8ab-b183e0ed78bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.977\n",
            "Accuracy on test set: 0.971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost and Random Forests are both powerful algorithms for classification and regression tasks. XGBoost is a gradient boosting method that optimizes a differentiable loss function using an ensemble of weak decision trees. Random Forests, on the other hand, are an ensemble of decision trees that are trained independently and combined to make predictions. Both algorithms have their strengths and weaknesses, and the choice between them often depends on the specific problem at hand."
      ],
      "metadata": {
        "id": "7hdWJLeY3sfj"
      }
    }
  ]
}